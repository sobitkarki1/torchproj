{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, n_heads, hidden_dim, n_layers, output_dim, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 5000, emb_dim))\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(emb_dim, n_heads, hidden_dim, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        src = self.dropout(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 10000  # Vocabulary size\n",
    "emb_dim = 512     # Embedding dimension\n",
    "n_heads = 8       # Number of attention heads\n",
    "hidden_dim = 2048 # Hidden dimension of feedforward layers\n",
    "n_layers = 6      # Number of transformer layers\n",
    "output_dim = 10000 # Output dimension (usually same as input_dim for language models)\n",
    "dropout = 0.1     # Dropout rate\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = TransformerModel(input_dim, emb_dim, n_heads, hidden_dim, n_layers, output_dim, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Example input (batch_size, sequence_length)\n",
    "example_input = torch.randint(0, input_dim, (32, 100))\n",
    "\n",
    "# Forward pass\n",
    "output = model(example_input)\n",
    "print(output.shape)  # Should be (batch_size, sequence_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
